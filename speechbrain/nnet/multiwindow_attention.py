import os
import math
import warnings
import torch
import torch.nn as nn
import collections.abc
from itertools import repeat
import torch.nn.functional as F
from typing import Optional, Tuple, Union, Any, List


def _trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are
    applied while sampling the normal with mean/std applied, therefore a, b args
    should be adjusted to match the range of mean, std args.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)


def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):
            return x
        return tuple(repeat(x, n))
    return parse


class Attention(nn.Module):
    def __init__(
            self,
            dim: int,
            num_heads: int = 8,
            qkv_bias: bool = False,
            attn_drop: float = 0.,
            proj_drop: float = 0.
            ):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.qkv_layer = nn.Linear(dim, dim*3, bias=qkv_bias)
        self.proj_layer = nn.Linear(dim, dim)
        self.attn_drop = nn.Dropout(attn_drop) if attn_drop != 0 else nn.Identity()
        self.proj_drop = nn.Dropout(proj_drop) if proj_drop != 0 else nn.Identity()

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv_layer(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute((2, 0, 3, 1, 4))
        q, k, v = qkv.unbind(0)
        attn = (q @ torch.swapaxes(k, -2, -1)) * self.scale
        attn = F.softmax(attn, dim=-1)
        attn = self.attn_drop(attn)
        x = torch.swapaxes((attn @ v), 1, 2).reshape(B, N, C)
        x = self.proj_layer(x)
        x = self.proj_drop(x)
        return x


def window_partition1d(x, window_size):
    B, W, C = x.shape
    x = x.reshape(B, W // window_size, window_size, C)
    # print(x.shape)
    windows = x.reshape(-1, window_size, C)
    return windows


def window_reverse1d(windows, window_size, W: int):
    B = int(windows.shape[0] / (W / window_size))
    if B == 0:
        # means originally input had a batch size of 1
        B = 1
    x = windows.reshape(B, W // window_size, window_size, -1)
    # print("in window_reverse1d, x.shape:", x.shape)
    x = x.reshape(B, W, -1)
    return x


def get_relative_position_index1d(win_w):
    coords_flatten = torch.stack(torch.meshgrid(torch.arange(win_w)))
    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Ww, Ww

    relative_coords = relative_coords.permute(1, 2, 0).contiguous()
    relative_coords[:, :, 0] += win_w - 1
    return relative_coords.sum(-1)


class WindowedAttentionHead(nn.Module):
    def __init__(self, 
                 head_dim: int,
                 window_size: int,
                 shift_windows: bool = False,
                 attn_drop: float = 0.,
                 ):
        super().__init__()
        self.head_dim = head_dim
        self.window_size = window_size
        self.shift_windows = shift_windows
        self.attn_drop = attn_drop
        self.relative_position_bias_table = nn.Parameter(torch.zeros(2*self.window_size-1, 1))
        self.register_buffer("relative_position_index", get_relative_position_index1d(self.window_size))

        self.scale = head_dim ** -0.5
        self.window_area = self.window_size * 1
        self.attn_drop = nn.Dropout(attn_drop)

        if self.shift_windows:
            self.shift_size = self.window_size // 2
        else:
            self.shift_size = 0

        trunc_normal_(self.relative_position_bias_table, std=.02)
    
    def _get_rel_pos_bias(self):
        relative_position_bias = self.relative_position_bias_table[
            self.relative_position_index.view(-1)].view(self.window_area, self.window_area, -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        return relative_position_bias

    def forward(self, q, k, v, key_padding_mask=None, attn_fill_value=-65000):
        # print("in windowedattentionhead forward with window_size: {}".format(self.window_size))
        # print("\t q.shape:{}, k.shape:{},v.shape:{}".format(q.shape, k.shape, v.shape))
        B, W, C = q.shape
        mask = None
        cnt = 0
        if self.shift_size > 0:
            img_mask = torch.zeros((1, W, 1))
            for w in (
                slice(0, -self.window_size),
                slice(-self.window_size, -self.shift_size),
                slice(-self.shift_size, None)):
                img_mask[:, w, :] = cnt
                cnt += 1
            mask_windows = window_partition1d(img_mask, self.window_size)
            mask_windows = mask_windows.reshape(-1, self.window_size)
            mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            mask = mask.masked_fill(mask != 0, float(-100.0)).masked_fill(mask == 0, float(0.0))

            q = torch.roll(q, shifts=-self.shift_size, dims=1)
            k = torch.roll(k, shifts=-self.shift_size, dims=1)
            v = torch.roll(v, shifts=-self.shift_size, dims=1)

        else:
            mask = None

        q = window_partition1d(q, self.window_size)
        k = window_partition1d(k, self.window_size)
        v = window_partition1d(v, self.window_size)

        attn = (q @ torch.swapaxes(k, -2, -1)) * self.scale
        attn = attn + self._get_rel_pos_bias()

        if mask is not None:
            B_, N, _ = attn.shape
            num_win = mask.shape[0]
            attn = attn.reshape(B_//num_win, num_win, N, N) + mask[None, Ellipsis]
            attn = attn.reshape(-1, N, N)
            
            # attn = F.softmax(attn, dim=-1)
        # else:

        if key_padding_mask is not None:
            #window-fy the key_padding_mask
            key_padding_mask_windowed = key_padding_mask.reshape(B, W // self.window_size, self.window_size, 1)
            key_padding_mask_windowed = key_padding_mask_windowed.reshape(-1, self.window_size, 1)

            if not os.path.exists("attn_before_{}.pt".format(self.window_size)):
                torch.save(attn, "attn_before_{}.pt".format(self.window_size))
            attn = attn.masked_fill(
                key_padding_mask_windowed.view(attn.shape[0], 1, self.window_size), attn_fill_value,
            )
            
        attn = F.softmax(attn, dim=-1)
        
        attn = self.attn_drop(attn)
        x = (attn @ v)
        
        shifted_x = window_reverse1d(x, self.window_size, W=W)
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=self.shift_size, dims=1)
        else:
            x = shifted_x
        if key_padding_mask is not None:
            x.masked_fill_(key_padding_mask.unsqueeze(-1), 0.0)
        return x, attn


class AttentionHead(nn.Module):
    def __init__(self, head_dim, attn_drop) -> None:
        super().__init__()
        self.scale = head_dim ** -0.5
        self.attn_drop = nn.Dropout(attn_drop)
    
    def forward(self, q, k, v, key_padding_mask=None, attn_fill_value=-65000):
        B, W, C = q.shape
        attn = (q @ torch.swapaxes(k, -2, -1)) * self.scale
        if key_padding_mask is not None:
            attn = attn.masked_fill(
                key_padding_mask.view(attn.shape[0], 1, W), attn_fill_value,
            )
        attn = F.softmax(attn, dim=-1)
        attn = self.attn_drop(attn)
        x = (attn @ v)
        if key_padding_mask is not None:
            x.masked_fill_(key_padding_mask.unsqueeze(-1), 0.0)
        return x, attn
    

class WindowedMultiHeadAttention(nn.Module):
    """This class is an implementation of the Multi-Window Multi-Head Attention (MW-MHA) module as proposed in [1].

    Reference: [1] "Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners" https://openreview.net/pdf?id=Q53QLftNkA

    Arguments
    ----------
    dim : int
        input dimension.
    window_sizes : Union[list, tuple, int]
        List of window sizes for the MultiWindowMultiheadAttention module. 
        A window size of 0 results in a head with standard global attention.
    shift_windows : bool
        Whether to shift the windows or not.
    num_heads : int
        number of parallel attention heads.
    bias: bool
        add bias to output projection
    qkv_bias : bool
        add bias to qkv projection.
    attn_drop : float
        dropout value for attention output.
    proj_drop : float
        dropout rate for projection output.
    """
    def __init__(self, 
                 dim: int,
                 window_sizes: Union[list, tuple, int],
                 shift_windows: bool = False,
                 num_heads: int = 8,
                 bias: bool = True,
                 qkv_bias: bool = False,
                 attn_drop: float = 0.,
                 proj_drop: float = 0.,
                 ):
        super().__init__()
        self.dim = dim
        self.shift_windows = shift_windows
        self.window_sizes = window_sizes
        self.attn_drop = attn_drop
        self.proj_drop = proj_drop
        self.num_heads = num_heads
        self.head_dim = self.dim // self.num_heads
        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)

        if type(self.window_sizes) == int:
            window_sizes = _ntuple(self.num_heads)(self.window_sizes)
        else:
            window_sizes = self.window_sizes
            assert len(window_sizes) == self.num_heads
        
        self.attn_heads = nn.ModuleList()
        for i in range(self.num_heads):
            ws_i = window_sizes[i]
            if ws_i == 0:
                self.attn_heads.append(AttentionHead(self.head_dim, self.attn_drop))
            else:
                self.attn_heads.append(WindowedAttentionHead(
                    self.head_dim,
                    window_size=ws_i,
                    shift_windows=self.shift_windows,
                    attn_drop=self.attn_drop
                ))
        self.proj = nn.Linear(dim, dim, bias=bias)
        self.proj_drop = nn.Dropout(proj_drop)
        self.attn_fill_value = -65000
        

    def forward(self, x, key_padding_mask=None):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute((2, 3, 0, 1, 4))
        q, k, v = qkv
        o = []
        for i in range(self.num_heads):
            head_i, attn_i = self.attn_heads[i](q[i], k[i], v[i], key_padding_mask, self.attn_fill_value)
            head_i = head_i.unsqueeze(0)
            o.append(head_i)
        o = torch.cat(o, dim=0)
        o = o.permute((1, 2, 0, 3)).reshape(B, N, -1)
        o = self.proj(o)
        o = self.proj_drop(o)
        return o, None


class MultiWindowMultiheadAttention(nn.Module):
    """
    This class is a wrapper over the WindowedMultiHeadAttention allowing for easy integration with SpeechBrain

    Arguments
    ----------
    nhead : int
        parallel attention heads.
    d_model : int
        The size of the model layers.
    dropout : float
        a Dropout layer on attn_output_weights (default: 0.0).
    bias : bool
        add bias as to the output projection layer (default: True).
    add_bias_qkv : bool
        add bias to the qkv projection layer.
    mwmha_windows: List[int]
        List of window sizes for the different attention heads.

    Example
    -------
    >>> inputs = torch.rand([8, 60, 512])
    >>> net = MultiWindowMultiheadAttention(nhead=4, d_model=inputs.shape[-1], 
                                            window_sizes=[3, 5, 10, 15])
    >>> outputs, attn = net(inputs, inputs, inputs)
    >>> outputs.shape
    torch.Size([8, 60, 512])
    """

    def __init__(
        self,
        nhead,
        d_model,
        dropout=0.0,
        bias=True,
        add_bias_qkv=False,
        mwmha_windows: Optional[List[int]] = []
    ):
        super().__init__()
        self.attn = WindowedMultiHeadAttention(
            dim=d_model,
            window_sizes=mwmha_windows,
            shift_windows=False,
            num_heads=nhead,
            bias=bias,
            qkv_bias=add_bias_qkv,
            attn_drop=dropout
        )

    def forward(
        self,
        query,
        key,
        value,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None,
        return_attn_weights: Optional[torch.Tensor] = True,
        pos_embs: Optional[torch.Tensor] = None,
    ):
        """
        Arguments
        ----------
        query : torch.Tensor
            (B, L, E) where L is the target sequence length,
            B is the batch size, E is the embedding dimension.
        key : torch.Tensor
            (B, S, E) where S is the source sequence length,
            B is the batch size, E is the embedding dimension.
        value : torch.Tensor
            (B, S, E) where S is the source sequence length,
            B is the batch size, E is the embedding dimension.
        key_padding_mask : torch.Tensor, optional
            (B, S) where B is the batch size, S is the source sequence
            length. If a ByteTensor is provided, the non-zero positions will
            be ignored while the position with the zero positions will be
            unchanged. If a BoolTensor is provided, the positions with the
            value of True will be ignored while the position with the value
            of False will be unchanged.
        attn_mask : torch.Tensor, optional
            2D mask (L, S) where L is the target sequence length, S is
            the source sequence length.
            3D mask (N*num_heads, L, S) where N is the batch
            size, L is the target sequence length, S is the source sequence
            length. attn_mask ensure that position i is allowed to attend the
            unmasked positions. If a ByteTensor is provided, the non-zero
            positions are not allowed to attend while the zero positions will
            be unchanged. If a BoolTensor is provided, positions with True is
            not allowed to attend while False values will be unchanged. If a
            FloatTensor is provided, it will be added to the attention weight.
        pos_embs: torch.Tensor, optional
            Positional embeddings added to the attention map of shape (L, S, E) or (L, S, 1).

        Outputs
        -------
        attn_output : torch.Tensor
            (B, L, E) where L is the target sequence length, B is the
            batch size, E is the embedding dimension.
        attn_output_weights : torch.Tensor
            (B, L, S) where B is the batch size, L is the target
            sequence length, S is the source sequence length.
        """
        # this will be legit because of https://github.com/pytorch/pytorch/blob/5288d05cfdda85c46c4df84617fa7f37c21b10b3/torch/nn/functional.py#L4946
        # we can inject relative learnable pos embeddings directly in MHA via the attn_mask
        if pos_embs is not None:
            if attn_mask is not None:
                attn_mask += pos_embs
            else:
                attn_mask = pos_embs
        
        if (query is key or torch.equal(query, key)) and (
                key is value or torch.equal(key, value)
            ):
            output, _ = self.attn(query, key_padding_mask)

            
        else:
            raise NotImplementedError
        if return_attn_weights:
            return output, None
        else:
            return output
